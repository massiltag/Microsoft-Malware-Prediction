import pickle
import pandas as pd
import numpy as np
import tensorflow as tf


def csv_to_pickled_converter():
    """
    convert csv data to pickle
    :return: nothing
    """
    df = pd.read_csv("../data/csv/test.csv")
    df.to_pickle("../data/pickled/test.pkl")
    print("test file converted")
    df = pd.read_csv("../data/csv/train.csv")
    df.to_pickle("../data/pickled/train.pkl")

    df = df.iloc[len(df)//4, :]
    df.to_pickle("../data/pickled/mini_train.pkl")

    df = df.iloc[:100000, :]
    df.to_pickle("../data/pickled/tiny_train.pkl")


def categorical_conversion(df_input):
    """
    Convert to categorical instead of object
    Non categorical (non numeric, i.e. dtype = object) data need to be converted for TF 2.0 to works.
    Example : if a column has 3 possible values like Windows10, Windows7, WindowsXP, convert to 0, 1, 2

    """

    for i in df_input.columns:
        if df_input[i].dtype == "object":
            df_input[i] = pd.Categorical(df_input[i])
            df_input[i] = df_input[i].cat.codes

    return df_input


def get_compiled_model():
    model = tf.keras.Sequential([
        # l1/l2 Regularization: techniques intended to discourage complexity of a model by penalizing the loss function.
        tf.keras.layers.Dense(80, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(20, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(20, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(1)
    ])

    model.compile(optimizer='adam',
    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # permet de changer le learning rate
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    return model


def neural_net():
    # Création d'un réseau de neurones
    target = df.pop('HasDetections')
    tf.keras.backend.set_floatx('float64')  # utilise des float64 au lieu de 32
    dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))  # read the values from a pandas dataframe
    train_dataset = dataset.shuffle(len(df)).batch(1)  # shuffle and batch the dataset
    # train the model
    model = get_compiled_model()
    model.fit(train_dataset, epochs=15, use_multiprocessing=True)


if __name__ == '__main__':
    # execute this to convert to pickle for best performances. You need this for the code below to work
    # csv_to_pickled_converter()

    df = pd.read_pickle("../data/pickled/tiny_train.pkl")  # read a smaller dataset so we can work faster

    # defines MachineIdentifier as index, i.e. defines machine hash as index because unique
    df.set_index("MachineIdentifier", inplace=True)


    # NaN drop
    df.drop(columns=[
        "AutoSampleOptIn", "Census_InternalBatteryNumberOfCharges", "Census_InternalBatteryType",
        "Census_IsFlightingInternal", "Census_IsFlightsDisabled", "Census_IsWIMBootEnabled", "Census_ProcessorClass",
        "Census_ThresholdOptIn", "DefaultBrowsersIdentifier", "IsBeta", "ProductName", "PuaMode", "UacLuaenable"], inplace=True)

    # columns that have more than 5 000 different values, cf.doc:
    numerical_features = [
        "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_PrimaryDiskTotalCapacity",
        "Census_ProcessorCoreCount", "Census_SystemVolumeTotalCapacity", "Census_TotalPhysicalRAM"
    ]

    # column with binary (0 or 1) values
    binary_features = [
        "Census_HasOpticalDiskDrive", "Census_IsAlwaysOnAlwaysConnectedCapable", "Census_IsPenCapable",
        "Census_IsPortableOperatingSystem", "Census_IsSecureBootEnabled", "Census_IsTouchEnabled",
        "Census_IsVirtualDevice", "Firewall", "HasTpm", "IsProtected", "IsSxsPassiveMode", "SMode", "WdftIsGamer"
    ]

    # replace NaN with median for each column where NaN are present
    for i in df.columns:
        s = df.loc[:, i]
        if s.isna().sum() != 0:

            if i in numerical_features:  # set NaNs in numerical features to -1
                s.fillna(-1, inplace=True)

            elif i in binary_features:  # set NaNs in binary feature to the most frequent one
                s.fillna(s.mode().iloc[0], inplace=True)

            else:
                s = s.astype(str)
                s = s.str.lower()
                s.fillna("unknown", inplace=True)

            df[i] = s

    df = categorical_conversion(df)  # convert data to categorical if the column dtype is "object"



    # création et train du NN
    neural_net()

