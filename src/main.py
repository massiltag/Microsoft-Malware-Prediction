import pickle
import pandas as pd
import numpy as np
import tensorflow as tf
import datetime as dt
import matplotlib.pyplot as plt
from sklearn import datasets, preprocessing
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from xgboost import plot_tree
from scipy.spatial import distance
from sklearn.model_selection import train_test_split as tts
from xgboost.sklearn import XGBClassifier
from sklearn import neighbors

import seaborn as sns
sns.set()  # affichage des plots plus joli


def csv_to_pickled_converter():
    """
    convert csv data files to pickle
    :return: nothing
    """
    df = pd.read_csv("../data/csv/test.csv")
    df.to_pickle("../data/pickled/test.pkl")
    print("test file converted")
    df = pd.read_csv("../data/csv/train.csv")
    df.to_pickle("../data/pickled/train.pkl")

    df = df.iloc[len(df)//4, :]
    df.to_pickle("../data/pickled/mini_train.pkl")

    df = df.iloc[:100000, :]
    df.to_pickle("../data/pickled/tiny_train.pkl")


def cleaning_df(original_df: pd.DataFrame) -> pd.DataFrame:
    df: pd.DataFrame = original_df.copy()
    # drop cols with too much NaN
    df.drop(columns=[
        "AutoSampleOptIn", "Census_InternalBatteryNumberOfCharges", "Census_InternalBatteryType",
        "Census_IsFlightingInternal", "Census_IsFlightsDisabled", "Census_IsWIMBootEnabled", "Census_ProcessorClass",
        "Census_ThresholdOptIn", "DefaultBrowsersIdentifier", "IsBeta", "ProductName", "PuaMode", "UacLuaenable"],
        inplace=True)
    # columns with numerical values:
    numerical_features = [
        "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_PrimaryDiskTotalCapacity",
        "Census_ProcessorCoreCount", "Census_SystemVolumeTotalCapacity", "Census_TotalPhysicalRAM"
    ]
    # columns with binary (0 or 1) values:
    binary_features = [
        "Census_HasOpticalDiskDrive", "Census_IsAlwaysOnAlwaysConnectedCapable", "Census_IsPenCapable",
        "Census_IsPortableOperatingSystem", "Census_IsSecureBootEnabled", "Census_IsTouchEnabled",
        "Census_IsVirtualDevice", "Firewall", "HasTpm", "IsProtected", "IsSxsPassiveMode", "SMode", "Wdft_IsGamer"
    ]
    # replace NaN
    for i in df.columns:
        s: pd.Series = df.loc[:, i]
        if i in numerical_features:  # set NaNs in numerical features to -1
            s.fillna(-1, inplace=True)
        elif i in binary_features:  # set NaNs in binary feature to the most frequent one
            s.fillna(s.mode().iloc[0], inplace=True)
        else:  # set to -1 if numerical, else to "unknown"
            if s.dtype == "object":
                s = s.astype(str)
                s = s.str.lower()
                s.fillna("unknown", inplace=True)
                s.replace(to_replace="nan", value="unknown", inplace=True)
                s[s.value_counts(normalize=True)[s].values < 0.05] = "unknown"  # replace low frequency values
            else:
                s.fillna(-1, inplace=True)

        df[i] = s.values
        if df[i].dtype == "int64" or df[i].dtype == "float64":
            df.loc[df[i].value_counts(normalize=True)[df[i]].values < 0.05, i] = -1
    df = categorical_conversion(df)  # convert data to categorical if the column dtype is "object"
    cols = df.columns
    index = df.index

    # normalization
    x = df.values
    min_max_scaler = preprocessing.MinMaxScaler()
    x = min_max_scaler.fit_transform(x)
    df = pd.DataFrame(x, columns=cols, index=index)
    return df


def categorical_conversion(df_input: pd.DataFrame) -> pd.DataFrame:
    """
    Convert to categorical instead of object
    Non categorical (non numeric, i.e. dtype = object) data need to be converted for TF 2.0 to works.
    Example : if a column has 3 possible values like Windows10, Windows7, WindowsXP, convert to 0, 1, 2

    """

    for c in df_input.columns:
        if df_input[c].dtype == "object":
            df_input[c] = pd.Categorical(df_input[c])
            df_input[c] = df_input[c].cat.codes

    return df_input


def neural_net(original_df: pd.DataFrame, model_number: int, nb_iterations=15) -> tf.keras.Sequential:
    """
    Creation du reseau de neurones
    :param original_df: la DataFrame sur laquelle entrainer le NN
    :param model_number: le numero de modele souhaite (actuellement 3)
    :param nb_iterations: le nombre d'epoch souhaitees
    :return: le modele entraine
    """

    df_copy = original_df.copy()

    target = df_copy.pop('HasDetections')
    tf.keras.backend.set_floatx('float64')  # use float64 instead of 32
    dataset = tf.data.Dataset.from_tensor_slices((df_copy.values, target.values))  # read values from a pandas dataframe
    train_dataset = dataset.shuffle(len(df_copy)).batch(64)  # shuffle and batch dataset
    # train the model
    model = get_compiled_model(model_number)
    model.fit(train_dataset, epochs=nb_iterations, use_multiprocessing=True)

    return model


def get_compiled_model(model_number: int) -> tf.keras.Sequential:
    """
    Creer le modele approprie. Actuellement, 3 modeles existent
    :param model_number: le numero du modele souhaite
    :return: le modele entraine pour realiser des predictions
    """
    if model_number == 1:
        model: tf.keras.Sequential = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='relu', input_shape=(68, )),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(16, activation='relu'),
            tf.keras.layers.Dense(8, activation='relu'),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # change learning rate
        model.compile(optimizer='adam',
                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                      metrics=['accuracy'])

    elif model_number == 2:
        model: tf.keras.Sequential = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='sigmoid', input_shape=(68, )),
            tf.keras.layers.Dense(32, activation='sigmoid'),
            tf.keras.layers.Dense(16, activation='sigmoid'),
            tf.keras.layers.Dense(8, activation='sigmoid'),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # change learning rate
        model.compile(optimizer='adam',
                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                      metrics=['accuracy'])

    else:
        model: tf.keras.Sequential = tf.keras.Sequential([
            # l1/l2 Regularization:
            #   techniques intended to discourage complexity of a model by penalizing the loss function.
            # tf.keras.layers.Dense(64, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
            tf.keras.layers.Dense(64, activation='relu', input_shape=(68, )),
            tf.keras.layers.Dropout(0.1),
            # tf.keras.layers.Dense(32, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(16, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(8, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # change learning rate
        model.compile(optimizer='adam',
                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                      metrics=['accuracy'])
    return model


def decision_tree_algo(original_df: pd.DataFrame):
    """
    Mon propre test du decision tree pour essayer de l'ameliorer et proposer mes idees a Max.
    Je l'ai mis pour montrer comment j'ai fait, mais la fonction pourrait etre ameliorer
    :param original_df: la DF originale sur laquelle construire le decision tree
    :return: rien
    """
    copied_df = original_df.copy()

    data = copied_df.iloc[:, :-1]
    target = copied_df.iloc[:, -1]
    print()
    print("========================================================")
    print("========================================================")
    print("In decision tree algorithm.")
    d1 = dt.datetime.now()
    xtrain, xtest, ytrain, ytest = tts(data, target, train_size=0.8)
    boost = XGBClassifier(max_depth=4, n_estimators=500)
    boost.fit(xtrain, ytrain)
    boost_prediction = boost.predict(xtest)
    print("Score Train:", round(boost.score(xtest, ytest) * 100, 2), " %")
    d2 = dt.datetime.now()
    print("Took ", d2 - d1)
    print("End decision tree algorithm.")

    labels = 'Found', 'Not found'
    hamming = distance.hamming(ytest, boost_prediction)
    rates = [1 - hamming, hamming]
    fig1, ax1 = plt.subplots()
    ax1.pie(rates, labels=labels, autopct='%0.2f%%')
    plt.show()

    plot_tree(boost, rankdir='LR')
    fig = plt.gcf()
    fig.set_size_inches(150, 50)
    # fig.savefig("tree.png")
    plt.show()


def display_predictions(chosen_file: str, trained_model: tf.keras.Sequential, original_columns_list):
    """
    Fonction qui affiche les 10 premiers resultats comparant la prediction et la realite
    :param chosen_file: chemin du fichier a comparer
    :param trained_model: modele entraine pour afficher les predictions realisees
    :param original_columns_list: nom des colonnes dans la DF avant nettoyage des donnees
    :return: rien du tout
    """
    df_test = pd.read_pickle(chosen_file)  # predictions sur le fichier d'origine pour comparer
    df_test.columns = original_columns_list
    df_test.set_index("MachineIdentifier", inplace=True)
    df_test = cleaning_df(df_test)
    predictions = trained_model.predict(df_test.iloc[:, :-1])
    for i in range(0, 10):
        print("predicted infection: {:.2%} | actual outcome : {}".format(predictions[i][0], df_test.iloc[i, -1]))


def entrainement_complet(passed_df: pd.DataFrame, file_chosen: str, epoch_number: int, original_columns_list):
    """
    La fonction se chargeant de faire les 3 entrainements
    :param passed_df: la DF originale
    :param file_chosen: le nom du fichier choisit
    :param epoch_number: le nombre d'epoch pour le train du NN
    :param original_columns_list: le nom des colonnes avant nettoyage.
    :return: rien
    """
    copied_df = passed_df.copy()
    # 1st Model of Deep Neural Network
    print("================= MODEL 1 =================")
    retrieved_model = neural_net(copied_df, 1, epoch_number)
    display_predictions(file_chosen, retrieved_model, original_columns_list)
    print("===========================================")
    # 2nd Model of Deep Neural Network
    print("================= MODEL 2 =================")
    retrieved_model = neural_net(copied_df, 2, epoch_number)
    display_predictions(file_chosen, retrieved_model, original_columns_list)
    print("===========================================")
    # 3rd Model of Deep Neural Network
    print("================= MODEL 3 =================")
    retrieved_model = neural_net(copied_df, 3, epoch_number)
    display_predictions(file_chosen, retrieved_model, original_columns_list)
    print("===========================================")


def programme_complet(file_chosen: str, epoch_number: int, do_conversion: bool = False):
    """
    Le programme complet executant tout notre projet
    :param file_chosen: chemin du dataset a ouvrir
    :param epoch_number: nombre d'epoch pour chaque NN a entrainer (le meme
    :param do_conversion: si il faut convertir les csv du dataset en pickle. Faux par defaut
    :return: nothing
    """

    # uncomment this to convert to pickle for best performances if you do not have the pickles.
    # You need pickles for the code below to work
    if do_conversion:
        csv_to_pickled_converter()

    # file_chosen = file_path
    df = pd.read_pickle(file_chosen)  # read a smaller dataset so we can work faster
    # defines MachineIdentifier as index, i.e. defines machine hash as index because unique
    original_columns_list = df.columns  # sert apres a trouver ajouter le nom des colonnes pour le fichier test

    df.set_index("MachineIdentifier", inplace=True)

    # used to have a multiple of 64 (the batch size)
    if file_chosen == "../data/pickled/tiny_train.pkl":
        pass
    elif file_chosen == "../data/pickled/mini_train.pkl":
        df = df.iloc[:2230336, :]
    else:  # if train.pkl:
        df = df.iloc[:8921472, :]
    df = cleaning_df(df)  # nettoyage des donnees de la DF

    entrainement_complet(df, file_chosen, epoch_number, original_columns_list)


if __name__ == '__main__':
    programme_complet("../data/pickled/tiny_train.pkl", 1)  # changer en train.pkl pour le dataset complet
