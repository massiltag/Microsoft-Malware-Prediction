import pickle
import pandas as pd
import numpy as np
import tensorflow as tf


def csv_to_pickled_converter():
    """
    convert csv data to pickle
    :return: nothing
    """
    df = pd.read_csv("../data/csv/test.csv")
    df.to_pickle("../data/pickled/test.pkl")
    print("test file converted")
    df = pd.read_csv("../data/csv/train.csv")
    df.to_pickle("../data/pickled/train.pkl")

    df = df.iloc[len(df)//4, :]
    df.to_pickle("../data/pickled/mini_train.pkl")
    print("train file converted")


def display_column_info():
    df = pd.read_pickle("../data/pickled/train.pkl")
    nb_entry = len(df.index)
    # bloqué à colonnes 8, 14, 15, 28, 29, 31, 41, 52, 53, 68 --> raison : colonne est "object"
    for i in range(69, len(df.columns)):
        temp = df.iloc[:, i]
        print("i:", i)
        if len(set(temp.tolist())) < 100:
            print("True")
            print(i, ")", df.columns[i], ": ", set(temp))
            print()
    print("\nFIN")


def get_dataset(file_path, LABEL_COLUMN, **kwargs):
    dataset = tf.data.experimental.make_csv_dataset(
        file_path,
        batch_size=5,  # Artificially small to make examples easier to show.
        label_name=LABEL_COLUMN,
        na_value="?",
        num_epochs=1,
        ignore_errors=True,
        **kwargs)
    return dataset


def categorical_conversion(df_input):
    """
    Convert to categorical instead of object
    Non categorical (non numeric, i.e. dtype = object) data need to be converted for TF 2.0 to works.
    Example : if a column has 3 possible values like Windows10, Windows7, WindowsXP, convert to 0, 1, 2

    """

    for i in df_input.columns:
        if df_input[i].dtype == "object":
            df_input[i] = pd.Categorical(df_input[i])
            df_input[i] = df_input[i].cat.codes

    return df_input


def get_compiled_model():
    model = tf.keras.Sequential([
        # Regularization: technique intended to discourage complexity of a model by penalizing the loss function.
        tf.keras.layers.Dense(70, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(50, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(50, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(20, activation='relu'),
        tf.keras.layers.Dense(1)
    ])

    model.compile(optimizer='adam',
    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # permet de changer le learning rate
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    return model


def neural_net():
    # Création d'un réseau de neurones
    target = df.pop('HasDetections')
    tf.keras.backend.set_floatx('float64')  # utilise des float64 au lieu de 32
    dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))  # read the values from a pandas dataframe
    train_dataset = dataset.shuffle(len(df)).batch(1)  # shuffle and batch the dataset
    # train the model
    model = get_compiled_model()
    model.fit(train_dataset, epochs=15, use_multiprocessing=True)


if __name__ == '__main__':
    # execute this to convert to pickle for best performances. You need this for the code below to work
    # csv_to_pickled_converter()

    # display_column_info()      # execute this to display info about columns

    # df = pd.read_pickle("../data/pickled/tiny_train.pkl")  # read a smaller dataset so we can work faster
    df = pd.read_pickle("../data/pickled/tiny_train.pkl")  # read a smaller dataset so we can work faster

    # defines MachineIdentifier as index, i.e. defines machine hash as index because it is unique and no machines are present multiple times
    df.set_index("MachineIdentifier", inplace=True)


    # NaN drop
    #                               > 95%                        >83%                        > 60%                   > 60%                    > 30%
    # df.drop(columns=["DefaultBrowsersIdentifier", "Census_IsFlightingInternal", "Census_ThresholdOptIn", "Census_IsWIMBootEnabled", "OrganizationIdentifier"], inplace=True)
    df.drop(columns=["DefaultBrowsersIdentifier", "Census_IsFlightingInternal", "Census_ThresholdOptIn", "Census_IsWIMBootEnabled"], inplace=True)

    # columns that have more than 5 000 different values, cf.doc:
    columns_with_constant = [
        "AvSigVersion",
        "AVProductStatesIdentifier",
        "CityIdentifier",
        "Census_OEMModelIdentifier",
        "Census_PrimaryDiskTotalCapacity",
        "Census_SystemVolumeTotalCapacity",
        "Census_InternalBatteryNumberOfCharges",
        "Census_FirmwareVersionIdentifier",
    ]

    df = categorical_conversion(df)  # convert data to categorical if the column dtype is "object"
    df.drop(columns=columns_with_constant, inplace=True)

    # drop columns that does not seem interesting
    columns_not_interesting = [
        "Census_InternalPrimaryDiagonalDisplaySizeInInches",
        "Census_InternalPrimaryDisplayResolutionHorizontal",
        "Census_InternalPrimaryDisplayResolutionVertical",
        "CensusInternalBatteryType",
        "CensusIsFlightingInternal",
        "CensusIsFlightsDisabled",
        "CensusIsWIMBootEnabled",
        "CensusProcessorClass",
        "IsBeta",
        "ProductName",
        "PuaMode",
        "UacLuaenable",
    ]
    # affiche les colonnes où des NaN sont présents
    # for i in df.columns:
    #     s = df.loc[:, i]
    #     if (s.isna().sum())/len(df) > 0.1:
    #         print("column ", i, ":", s.isna().sum(), " -->", ((s.isna().sum())/len(df))*100, "%, median =", s.median(skipna=True))

    # replace NaN with median for each column where NaN are present
    for i in df.columns:
        s = df.loc[:, i]
        if s.isna().sum() != 0:
            series_median = s.median(skipna=True)
            s.fillna(series_median, inplace=True)
            df[i] = s


    # # print(set(list(df["Wdft_IsGamer"].values)))
    # data_set = set(list(df["Wdft_IsGamer"].values))
    # print(data_set)
    # exit(0)


    # number different values in each columns (if number is too high of too low the column is not interesting)
    # for i in df.columns:
    #     print("-", i, ":", len(set(df[i].values)))



    # création et train du NN
    neural_net()

