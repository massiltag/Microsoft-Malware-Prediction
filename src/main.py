import pickle
import pandas as pd
import numpy as np
import tensorflow as tf
import datetime as dt

import matplotlib.pyplot as plt

from sklearn import datasets, preprocessing
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from xgboost import plot_tree
from scipy.spatial import distance
from sklearn.model_selection import train_test_split as tts
from xgboost.sklearn import XGBClassifier

from sklearn import neighbors

import seaborn as sns
sns.set()


def csv_to_pickled_converter():
    """
    convert csv data to pickle
    :return: nothing
    """
    df = pd.read_csv("../data/csv/test.csv")
    df.to_pickle("../data/pickled/test.pkl")
    print("test file converted")
    df = pd.read_csv("../data/csv/train.csv")
    df.to_pickle("../data/pickled/train.pkl")

    df = df.iloc[len(df)//4, :]
    df.to_pickle("../data/pickled/mini_train.pkl")

    df = df.iloc[:100000, :]
    df.to_pickle("../data/pickled/tiny_train.pkl")


def categorical_conversion(df_input):
    """
    Convert to categorical instead of object
    Non categorical (non numeric, i.e. dtype = object) data need to be converted for TF 2.0 to works.
    Example : if a column has 3 possible values like Windows10, Windows7, WindowsXP, convert to 0, 1, 2

    """

    for c in df_input.columns:
        if df_input[c].dtype == "object":
            df_input[c] = pd.Categorical(df_input[c])
            df_input[c] = df_input[c].cat.codes

    return df_input


def get_compiled_model(model_number):
    # TODO: modifier nb layers, pas trop de souci avec overfitting (bcp de données),
    #  changer hyperparamètres pour vérifier qu'ils sont les bons,
    #  faire des métriques --> KPI,
    #  regarder si je peux mes dimensions pour le Deep NN ==> bien pour tout le monde.
    if model_number == 1:
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='relu', input_shape=(68, )),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(16, activation='relu'),
            tf.keras.layers.Dense(8, activation='relu'),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # change learning rate
        model.compile(optimizer='adam',
                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                      metrics=['accuracy'])

    elif model_number == 2:
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='sigmoid', input_shape=(68, )),
            tf.keras.layers.Dense(32, activation='sigmoid'),
            tf.keras.layers.Dense(16, activation='sigmoid'),
            tf.keras.layers.Dense(8, activation='sigmoid'),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # change learning rate
        model.compile(optimizer='adam',
                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                      metrics=['accuracy'])

    else:
        model = tf.keras.Sequential([
            # l1/l2 Regularization: techniques intended to discourage complexity of a model by penalizing the loss function.
            # tf.keras.layers.Dense(64, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
            tf.keras.layers.Dense(64, activation='relu', input_shape=(68, )),
            tf.keras.layers.Dropout(0.1),
            # tf.keras.layers.Dense(32, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(16, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(8, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # change learning rate
        model.compile(optimizer='adam',
                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                      metrics=['accuracy'])
    return model


def neural_net(original_df, model_number, nb_iterations=15):
    # creation of NN

    df_copy = original_df.copy()

    target = df_copy.pop('HasDetections')
    tf.keras.backend.set_floatx('float64')  # use float64 instead of 32
    dataset = tf.data.Dataset.from_tensor_slices((df_copy.values, target.values))  # read values from a pandas dataframe
    train_dataset = dataset.shuffle(len(df)).batch(64)  # shuffle and batch dataset
    # train the model
    model = get_compiled_model(model_number)
    model.fit(train_dataset, epochs=nb_iterations, use_multiprocessing=True)

    return model


def decision_tree_algo():
    data = df.iloc[:, :-1]
    target = df.iloc[:, -1]
    print()
    print("========================================================")
    print("========================================================")
    print("In decision tree algorithm.")
    d1 = dt.datetime.now()
    xtrain, xtest, ytrain, ytest = tts(data, target, train_size=0.8)
    boost = XGBClassifier(max_depth=4, n_estimators=500)
    boost.fit(xtrain, ytrain)
    boost_prediction = boost.predict(xtest)
    print("Score Train:", round(boost.score(xtest, ytest) * 100, 2), " %")
    d2 = dt.datetime.now()
    print("Took ", d2 - d1)
    print("End decision tree algorithm.")

    labels = 'Found', 'Not found'
    hamming = distance.hamming(ytest, boost_prediction)
    rates = [1 - hamming, hamming]
    fig1, ax1 = plt.subplots()
    ax1.pie(rates, labels=labels, autopct='%0.2f%%')
    plt.show()

    plot_tree(boost, rankdir='LR')
    fig = plt.gcf()
    fig.set_size_inches(150, 50)
    # fig.savefig("tree.png")
    plt.show()


def cleaning_df(original_df):
    df = original_df.copy()
    # drop cols with too much NaN
    df.drop(columns=[
        "AutoSampleOptIn", "Census_InternalBatteryNumberOfCharges", "Census_InternalBatteryType",
        "Census_IsFlightingInternal", "Census_IsFlightsDisabled", "Census_IsWIMBootEnabled", "Census_ProcessorClass",
        "Census_ThresholdOptIn", "DefaultBrowsersIdentifier", "IsBeta", "ProductName", "PuaMode", "UacLuaenable"],
        inplace=True)
    # columns with numerical values:
    numerical_features = [
        "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_PrimaryDiskTotalCapacity",
        "Census_ProcessorCoreCount", "Census_SystemVolumeTotalCapacity", "Census_TotalPhysicalRAM"
    ]
    # columns with binary (0 or 1) values:
    binary_features = [
        "Census_HasOpticalDiskDrive", "Census_IsAlwaysOnAlwaysConnectedCapable", "Census_IsPenCapable",
        "Census_IsPortableOperatingSystem", "Census_IsSecureBootEnabled", "Census_IsTouchEnabled",
        "Census_IsVirtualDevice", "Firewall", "HasTpm", "IsProtected", "IsSxsPassiveMode", "SMode", "Wdft_IsGamer"
    ]
    # replace NaN
    for i in df.columns:
        s = df.loc[:, i]
        if i in numerical_features:  # set NaNs in numerical features to -1
            s.fillna(-1, inplace=True)
        elif i in binary_features:  # set NaNs in binary feature to the most frequent one
            s.fillna(s.mode().iloc[0], inplace=True)
        else:  # set to -1 if numerical, else to "unknown"
            if s.dtype == "object":
                s = s.astype(str)
                s = s.str.lower()
                s.fillna("unknown", inplace=True)
                s.replace(to_replace="nan", value="unknown", inplace=True)
                s[s.value_counts(normalize=True)[s].values < 0.05] = "unknown"  # replace low frequency values
            else:
                s.fillna(-1, inplace=True)

        df[i] = s.values
        if df[i].dtype == "int64" or df[i].dtype == "float64":
            df.loc[df[i].value_counts(normalize=True)[df[i]].values < 0.05, i] = -1
    df = categorical_conversion(df)  # convert data to categorical if the column dtype is "object"
    cols = df.columns
    index = df.index
    # normalization
    X = df.values
    min_max_scaler = preprocessing.MinMaxScaler()
    X = min_max_scaler.fit_transform(X)
    df = pd.DataFrame(X, columns=cols, index=index)
    return df


def display_predictions(chosen_file, trained_model):
    """
    Fonction qui affiche les 10 premiers resultats comparant la prediction et la realite
    :param trained_model:
    :param chosen_file: le chemin du fichier a comparer
    :return: rien du tout
    """
    df_test = pd.read_pickle(chosen_file)  # predictions sur le fichier d'origine pour comparer
    df_test.columns = original_columns_list
    df_test.set_index("MachineIdentifier", inplace=True)
    df_test = cleaning_df(df_test)
    predictions = trained_model.predict(df_test.iloc[:, :-1])
    for i in range(0, 10):
        print("predicted infection: {:.2%} | actual outcome : {}".format(predictions[i][0], df_test.iloc[i, -1]))


if __name__ == '__main__':
    # TODO: sortir les trucs du main qui on rien à y faire
    # execute this to convert to pickle for best performances if you do not have the pickles.
    # You need this for the code below to work
    # csv_to_pickled_converter()

    file_chosen = "../data/pickled/train.pkl"
    df = pd.read_pickle(file_chosen)  # read a smaller dataset so we can work faster

    # defines MachineIdentifier as index, i.e. defines machine hash as index because unique
    original_columns_list = df.columns  # sert apres a trouver ajouter le nom des colonnes pour le fichier test

    df.set_index("MachineIdentifier", inplace=True)
    # used for take a multiple of 64, the batch size
    if file_chosen == "../data/pickled/tiny_train.pkl":
        pass
    elif file_chosen == "../data/pickled/mini_train.pkl":
        df = df.iloc[:2230336, :]
    else:  # if train.pkl:
        df = df.iloc[:8921472, :]

    df = cleaning_df(df)  # nettoyage des donnees de la DF

    # y = df.pop('HasDetections').to_numpy()

    # column to normaliser
    # x1 = df[["Census_PrimaryDiskTotalCapacity"]].values.astype(float)
    # x2 = df[["AVProductStatesIdentifier"]].values.astype(float)
    # x3 = df[["OsBuild"]].values.astype(float)
    # x4 = df[["Census_OSBuildNumber"]].values.astype(float)
    #
    # min_max_scaler = preprocessing.MinMaxScaler()
    # x1 = min_max_scaler.fit_transform(x1)
    # df["Census_PrimaryDiskTotalCapacity"] = x1
    #
    # x2 = min_max_scaler.fit_transform(x2)
    # df["AVProductStatesIdentifier"] = x2
    #
    # x3 = min_max_scaler.fit_transform(x3)
    # df["OsBuild"] = x3
    #
    # x4 = min_max_scaler.fit_transform(x4)
    # df["Census_OSBuildNumber"] = x4

    # creation and train of NN

    # 1st Model of Deep Neural Network
    print("================= MODEL 1 =================")
    model = neural_net(df, 1, 15)
    display_predictions(file_chosen, model)
    print("===========================================")

    # 2nd Model of Deep Neural Network
    print("================= MODEL 2 =================")
    model = neural_net(df, 2, 15)
    display_predictions(file_chosen, model)
    print("===========================================")


    # 3rd Model of Deep Neural Network
    print("================= MODEL 3 =================")
    model = neural_net(df, 3, 15)
    display_predictions(file_chosen, model)
    print("===========================================")

    """
    target_names = [0, 1]


    # PCA
    pca = PCA(n_components=2)
    X_r = pca.fit(X).transform(X)

    # LDA
    lda = LinearDiscriminantAnalysis(n_components=2)
    X_r2 = lda.fit(X, y).transform(X)

    # Percentage of variance explained for each components
    print('explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_))

    plt.figure()
    colors = ['navy', 'turquoise']
    lw = 2

    for color, i, target_name in zip(colors, [0, 1], target_names):
        if i == 0:
            plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=1, lw=lw, label=target_name)
        else:
            plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.05, lw=lw, label=target_name)
    plt.legend(loc='best', shadow=False, scatterpoints=1)
    plt.title('PCA of our dataset')

    # plt.figure()
    # for color, i, target_name in zip(colors, [0, 1], target_names):
    #     plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name)
    # plt.legend(loc='best', shadow=False, scatterpoints=1)
    # plt.title('LDA of our dataset')

    plt.show()
    """

    # decision tree : 65.05% avec train (fichier complet). 64.99% avec mini_train (approx. 2.2 millions rows)
    # decision_tree_algo()

    """
    # 9  NN: 100 000 -> 58.3% de précision: 100 000; 330 336 -> 58.3%
    # 10 NN: 330 336 -> 58.3%
    data = df.iloc[:, :-1]
    target = df.iloc[:, -1]

    print()
    print("========================================================")
    print("========================================================")
    print("In k-NN algorithm.")
    d1 = dt.datetime.now()

    xtrain, xtest, ytrain, ytest = tts(data, target, train_size=0.8)
    knn1 = neighbors.KNeighborsClassifier(n_neighbors=9)
    knn1.fit(xtrain, ytrain)
    print('Precision rate: %f' % knn1.score(xtest, ytest))

    d2 = dt.datetime.now()
    print("Took ", d2 - d1)
    print("End k-NN tree algorithm.")



    precision = []
    for k in range(2, 15):
        knn = neighbors.KNeighborsClassifier(k)
        precision.append(100 * (knn.fit(xtrain, ytrain).score(xtest, ytest)))
    plt.plot(range(2, 15), precision, 'o-')
    plt.show()
    """




