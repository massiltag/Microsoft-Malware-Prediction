import pickle
import pandas as pd
import numpy as np
import tensorflow as tf


def csv_to_pickled_converter():
    """
    convert csv data to pickle
    :return: nothing
    """
    df = pd.read_csv("../data/csv/test.csv")
    df.to_pickle("../data/pickled/test.pkl")
    print("test file converted")
    df = pd.read_csv("../data/csv/train.csv")
    df.to_pickle("../data/pickled/train.pkl")

    df = df.iloc[len(df)//4, :]
    df.to_pickle("../data/pickled/mini_train.pkl")

    df = df.iloc[:100000, :]
    df.to_pickle("../data/pickled/tiny_train.pkl")


def categorical_conversion(df_input):
    """
    Convert to categorical instead of object
    Non categorical (non numeric, i.e. dtype = object) data need to be converted for TF 2.0 to works.
    Example : if a column has 3 possible values like Windows10, Windows7, WindowsXP, convert to 0, 1, 2

    """

    for c in df_input.columns:
        if df_input[c].dtype == "object":
            df_input[c] = pd.Categorical(df_input[c])
            df_input[c] = df_input[c].cat.codes

    return df_input


def get_compiled_model():
    model = tf.keras.Sequential([
        # l1/l2 Regularization: techniques intended to discourage complexity of a model by penalizing the loss function.
        tf.keras.layers.Dense(70, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(20, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # change learning rate
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    return model


def neural_net():
    # creation of NN
    target = df.pop('HasDetections')
    tf.keras.backend.set_floatx('float64')  # utilise float64 instead of 32
    dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))  # read values from a pandas dataframe
    train_dataset = dataset.shuffle(len(df)).batch(1)  # shuffle and batch dataset
    # train the model
    model = get_compiled_model()
    model.fit(train_dataset, epochs=15, use_multiprocessing=True)


if __name__ == '__main__':
    # execute this to convert to pickle for best performances. You need this for the code below to work
    # csv_to_pickled_converter()

    df = pd.read_pickle("../data/pickled/tiny_train.pkl")  # read a smaller dataset so we can work faster

    # defines MachineIdentifier as index, i.e. defines machine hash as index because unique
    df.set_index("MachineIdentifier", inplace=True)

    # drop cols with too much NaN
    df.drop(columns=[
        "AutoSampleOptIn", "Census_InternalBatteryNumberOfCharges", "Census_InternalBatteryType",
        "Census_IsFlightingInternal", "Census_IsFlightsDisabled", "Census_IsWIMBootEnabled", "Census_ProcessorClass",
        "Census_ThresholdOptIn", "DefaultBrowsersIdentifier", "IsBeta", "ProductName", "PuaMode", "UacLuaenable"],
        inplace=True)

    # columns with numerical values:
    numerical_features = [
        "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Census_PrimaryDiskTotalCapacity",
        "Census_ProcessorCoreCount", "Census_SystemVolumeTotalCapacity", "Census_TotalPhysicalRAM"
    ]

    # columns with binary (0 or 1) values:
    binary_features = [
        "Census_HasOpticalDiskDrive", "Census_IsAlwaysOnAlwaysConnectedCapable", "Census_IsPenCapable",
        "Census_IsPortableOperatingSystem", "Census_IsSecureBootEnabled", "Census_IsTouchEnabled",
        "Census_IsVirtualDevice", "Firewall", "HasTpm", "IsProtected", "IsSxsPassiveMode", "SMode", "Wdft_IsGamer"
    ]

    # replace NaN
    for i in df.columns:
        s = df.loc[:, i]
        if i in numerical_features:  # set NaNs in numerical features to -1
            s.fillna(-1, inplace=True)
        elif i in binary_features:  # set NaNs in binary feature to the most frequent one
            s.fillna(s.mode().iloc[0], inplace=True)
        else:  # set to -1 if numerical, else to "unknown"
            if s.dtype == "object":
                s = s.astype(str)
                s = s.str.lower()
                s.fillna("unknown", inplace=True)
                s.replace(to_replace="nan", value="unknown", inplace=True)
                s[s.value_counts(normalize=True)[s].values < 0.05] = "unknown"  # replace low frequency values
            else:
                s.fillna(-1, inplace=True)

        df[i] = s.values
        if df[i].dtype == "int64" or df[i].dtype == "float64":
            df.loc[df[i].value_counts(normalize=True)[df[i]].values < 0.05, i] = -1

    df = categorical_conversion(df)  # convert data to categorical if the column dtype is "object"

    # creation and train of NN
    neural_net()
