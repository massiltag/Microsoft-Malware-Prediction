'''
    Microsoft Malware Prediction with XGBoost Decision Tree Algorithm ;

    Python Ver. 3.7.4 ;
    Requirements = [
        numpy,
        pandas,
        matplotlib,
        scipy,
        sklearn,
        xgboost,
        py-xgboost,
        python-graphviz
    };
'''
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json, datetime as dt

from sklearn.model_selection import train_test_split as tts
from xgboost.sklearn import XGBClassifier
from scipy.spatial import distance
from xgboost import plot_tree


def decision_tree():
    print("Microsoft Malware Prediction using a Decision Tree Algorithm (XGBoost)")
    d1 = dt.datetime.now()
    print("Data processing started at", "%02d:%02d" % (d1.hour, d1.minute))

    # Data loading
    with open('../../data/json/datatypes.json') as file:
        dtype = json.load(file)
    df = pd.read_csv('../../data/csv/microsoft-malware.csv', dtype=dtype)

    # Dropping categorical
    binary = []
    categorical = []
    numerical = []

    for key, value in dtype.items():
        if value in ['int8']:
            binary.append(key)
        if value in ['int16','category']:
            categorical.append(key)
        else:
            numerical.append(key)

    categorical.remove('MachineIdentifier') # Déjà enlevé par iloc
    df = df.drop(columns=list(categorical))


    # Cleaning NaN
    for i in df.columns:
        s = df.loc[:, i]
        if i in numerical:  # set NaNs in numerical features to -1
            s.fillna(-1, inplace=True)
        elif i in binary:  # set NaNs in binary feature to the most frequent one
            s.fillna(s.mode().iloc[0], inplace=True)
        df[i] = s.values
        if df[i].dtype == "int64" or df[i].dtype == "float64":
            df.loc[df[i].value_counts(normalize=True)[df[i]].values < 0.05, i] = -1


    # Splitting dataset
    data = df.iloc[:,1:-1] # Dropping MachineIdentifier & HasDetections
    target = df.iloc[:,-1] # Selecting HasDetections
    xtrain, xtest, ytrain, ytest = tts(data, target, train_size=0.8)


    # Training model
    boost = XGBClassifier(max_depth=2, n_estimators=200)
    boost.fit(xtrain, ytrain)
    boost_prediction = boost.predict(xtest)
    print("Score Train :", round(boost.score(xtest, ytest)*100, 2), " %")
    d2 = dt.datetime.now()
    print("Took ", d2-d1)


    # Plotting result
    labels = 'Found', 'Not found'
    hamming = distance.hamming(ytest, boost_prediction)
    rates = [1-hamming, hamming]
    fig1, ax1 = plt.subplots()
    ax1.pie(rates, labels=labels, autopct='%0.2f%%')
    plt.show()


    # Decision tree
    print('Plotting decision tree')
    plot_tree(boost, rankdir='LR')
    fig = plt.gcf()
    fig.set_size_inches(150,50)
    # fig.savefig("tree.png")
    plt.show()


if __name__=="__main__":
    decision_tree()

# Please check associated Jupyter notebook for performance study
